import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, StackingClassifier
import xgboost as xgb
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve
from sklearn.multiclass import OneVsRestClassifier
from sklearn.decomposition import PCA
from sklearn.preprocessing import label_binarize
from sklearn.metrics import auc
from sklearn.feature_selection import RFE, SelectKBest, f_classif
from sklearn.inspection import permutation_importance
from scipy.stats import randint, uniform
import pickle
import os
import warnings
import time

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
class EnhancedBearingDiagnosisModel:
    def __init__(self, features_path=None):
        """åˆå§‹åŒ–å¢å¼ºè¯Šæ–­æ¨¡å‹"""
        self.features_path = features_path or 'Final_selected_features_dataset.xlsx'
        self.features_df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.models = {}
        self.model_scores = {}
        self.best_model = None
        self.selected_features = None

        # åˆ›å»ºç»“æœæ–‡ä»¶å¤¹
        self.results_dir = "result-22"
        if not os.path.exists(self.results_dir):
            os.makedirs(self.results_dir)

    def load_features(self):
        """åŠ è½½ç‰¹å¾æ•°æ®"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            file_extension = os.path.splitext(self.features_path)[1].lower()

            if file_extension == '.xlsx':
                # è¯»å–Excelæ–‡ä»¶
                self.features_df = pd.read_excel(self.features_path)
            elif file_extension == '.csv':
                # å°è¯•å¤šç§ç¼–ç è¯»å–CSVæ–‡ä»¶
                encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312']
                for encoding in encodings:
                    try:
                        # æ·»åŠ æ›´å¤šå‚æ•°æ¥å¤„ç†CSVæ ¼å¼é—®é¢˜
                        self.features_df = pd.read_csv(
                            self.features_path,
                            encoding=encoding,
                            sep=',',  # æ˜ç¡®æŒ‡å®šåˆ†éš”ç¬¦
                            quotechar='"',  # æŒ‡å®šå¼•å·å­—ç¬¦
                            skipinitialspace=True,  # è·³è¿‡åˆ†éš”ç¬¦åçš„ç©ºæ ¼
                            engine='c'  # ä½¿ç”¨Cå¼•æ“ï¼Œæ›´å¿«æ›´å‡†ç¡®
                        )
                        print(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")
                        # éªŒè¯æ•°æ®æ˜¯å¦æ­£ç¡®è¯»å–ï¼ˆæ£€æŸ¥åˆ—æ•°ï¼‰
                        if len(self.features_df.columns) > 10:  # åº”è¯¥æœ‰å¾ˆå¤šåˆ—
                            break
                        else:
                            print(
                                f"è­¦å‘Š: ä½¿ç”¨ {encoding} ç¼–ç åªè¯»å–åˆ° {len(self.features_df.columns)} åˆ—ï¼Œç»§ç»­å°è¯•å…¶ä»–ç¼–ç ")
                            continue
                    except (UnicodeDecodeError, pd.errors.ParserError) as e:
                        print(f"ä½¿ç”¨ {encoding} ç¼–ç å¤±è´¥: {e}")
                        continue
                else:
                    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•æœ€åŸºæœ¬çš„è¯»å–æ–¹å¼
                    try:
                        self.features_df = pd.read_csv(
                            self.features_path,
                            encoding='utf-8',
                            sep=',',
                            engine='python',
                            error_bad_lines=False,
                            warn_bad_lines=True
                        )
                        print("ä½¿ç”¨åŸºæœ¬æ–¹å¼æˆåŠŸè¯»å–æ–‡ä»¶")
                    except Exception as e:
                        raise ValueError(f"æ— æ³•ä½¿ç”¨ä»»ä½•æ–¹å¼è¯»å–CSVæ–‡ä»¶: {e}")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")

            print(f"æˆåŠŸåŠ è½½ç‰¹å¾æ•°æ®ï¼Œå…± {len(self.features_df)} ä¸ªæ ·æœ¬")
            print(f"ç‰¹å¾æ•°é‡: {len(self.features_df.columns)}")
            if 'label' in self.features_df.columns:
                print(f"æ ‡ç­¾åˆ†å¸ƒ:\n{self.features_df['label'].value_counts()}")
            return True
        except Exception as e:
            print(f"åŠ è½½ç‰¹å¾æ•°æ®å¤±è´¥: {e}")
            return False

    def preprocess_data(self, test_size=0.3):
        """æ•°æ®é¢„å¤„ç†"""
        if self.features_df is None:
            print("è¯·å…ˆåŠ è½½ç‰¹å¾æ•°æ®ï¼")
            return False

        # é€‰æ‹©ç‰¹å¾åˆ—
        exclude_cols = ['label', 'filename', 'rpm', 'sampling_frequency', 'bearing_type', 'signal_length',
                        'sensor_position', 'data_source_path', 'data_source_directory', 'fault_severity',
                        'signal_duration_seconds']
        feature_cols = [col for col in self.features_df.columns if col not in exclude_cols]

        X = self.features_df[feature_cols].copy()
        y = self.features_df['label'].copy()

        # å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
        X = X.fillna(X.mean())
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.mean())

        # ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(y)

        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y_encoded, test_size=test_size, random_state=42, stratify=y_encoded
        )

        # ç‰¹å¾ç¼©æ”¾
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        # ä¿å­˜ç‰¹å¾åˆ—åï¼ˆæ•°æ®é›†å·²ç»è¿‡ç‰¹å¾é€‰æ‹©ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼‰
        self.selected_features = feature_cols

        print(f"\næ•°æ®é¢„å¤„ç†å®Œæˆ:")
        print(f"è®­ç»ƒé›†å¤§å°: {self.X_train_scaled.shape}")
        print(f"æµ‹è¯•é›†å¤§å°: {self.X_test_scaled.shape}")
        print(f"ä½¿ç”¨çš„ç‰¹å¾æ•°: {len(self.selected_features)}")

        return True

    def train_core_models(self):
        """è®­ç»ƒæ ¸å¿ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆåŒ…æ‹¬XGBoostå’ŒStackingèåˆæ¨¡å‹ï¼‰"""
        print("\n=== å¼€å§‹è®­ç»ƒæ ¸å¿ƒæ¨¡å‹ ===")
        
        # å®šä¹‰åŸºç¡€æ¨¡å‹
        base_models = {
            'Random_Forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            'Gradient_Boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=42
            ),
            'XGBoost': xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                eval_metric='mlogloss'
            ),
            'SVM_RBF': SVC(
                C=1.0,
                gamma='scale',
                kernel='rbf',
                probability=True,
                random_state=42
            ),
            'Logistic_Regression': LogisticRegression(
                C=1.0,
                penalty='l2',
                solver='liblinear',
                random_state=42,
                n_jobs=-1,
                max_iter=1000
            )
        }
        
        # åˆ›å»ºStackingé›†æˆæ¨¡å‹ï¼ˆèåˆéšæœºæ£®æ—ã€æ¢¯åº¦æå‡æ ‘ã€XGBoostï¼‰
        stacking_model = StackingClassifier(
            estimators=[
                ('rf', base_models['Random_Forest']),
                ('gb', base_models['Gradient_Boosting']),
                ('xgb', base_models['XGBoost'])
            ],
            final_estimator=LogisticRegression(random_state=42, max_iter=1000),
            cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
            stack_method='predict_proba',  # ä½¿ç”¨æ¦‚ç‡é¢„æµ‹è¿›è¡Œå †å 
            n_jobs=-1
        )
        
        # å°†æ‰€æœ‰æ¨¡å‹åˆå¹¶
        models = base_models.copy()
        models['Stacking_RF_GB_XGB'] = stacking_model
        
        self.models = {}
        self.model_scores = {}
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for name, model in models.items():
            print(f"\nè®­ç»ƒ {name}...")
            start_time = time.time()
            
            try:
                # è®­ç»ƒæ¨¡å‹
                model.fit(self.X_train_scaled, self.y_train)
                
                # é¢„æµ‹
                y_pred = model.predict(self.X_test_scaled)
                y_pred_proba = model.predict_proba(self.X_test_scaled)
                
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                accuracy = accuracy_score(self.y_test, y_pred)
                f1_macro = f1_score(self.y_test, y_pred, average='macro')
                f1_weighted = f1_score(self.y_test, y_pred, average='weighted')
                precision = precision_score(self.y_test, y_pred, average='macro')
                recall = recall_score(self.y_test, y_pred, average='macro')
                training_time = time.time() - start_time
                
                # ä¿å­˜æ¨¡å‹å’Œç»“æœ
                self.models[name] = model
                self.model_scores[name] = {
                    'accuracy': accuracy,
                    'f1_macro': f1_macro,
                    'f1_weighted': f1_weighted,
                    'precision': precision,
                    'recall': recall,
                    'predictions': y_pred,
                    'probabilities': y_pred_proba,
                    'training_time': training_time
                }
                
                print(f"  âœ“ {name} è®­ç»ƒå®Œæˆ:")
                print(f"    å‡†ç¡®ç‡: {accuracy:.4f}")
                print(f"    F1-macro: {f1_macro:.4f}")
                print(f"    F1-weighted: {f1_weighted:.4f}")
                print(f"    ç²¾ç¡®ç‡: {precision:.4f}")
                print(f"    å¬å›ç‡: {recall:.4f}")
                print(f"    è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
                
            except Exception as e:
                print(f"  âœ— {name} è®­ç»ƒå¤±è´¥: {str(e)}")
                continue
        
        # ç¡®å®šæœ€ä½³æ¨¡å‹
        if self.model_scores:
            best_model_name = max(self.model_scores.keys(), 
                                key=lambda x: self.model_scores[x]['f1_macro'])
            self.best_model = self.models[best_model_name]
            
            print(f"\nğŸ† åŸºç¡€è®­ç»ƒæœ€ä½³æ¨¡å‹: {best_model_name}")
            print(f"F1-macroå¾—åˆ†: {self.model_scores[best_model_name]['f1_macro']:.4f}")
        
        return True

    def optimize_random_forest_with_pso(self, n_particles=20, n_iterations=25):
        """ä½¿ç”¨PSOä¼˜åŒ–éšæœºæ£®æ—è¶…å‚æ•°"""
        print("\n=== å¼€å§‹PSOè¶…å‚æ•°ä¼˜åŒ– ===")
        
        # å®šä¹‰æœç´¢ç©ºé—´
        search_space = {
            "n_estimators": (50, 500),
            "max_depth": (5, 30),
            "min_samples_split": (2, 20),
            "min_samples_leaf": (1, 10)
        }
        
        def decode_particle(vec):
            """å°†ç²’å­å‘é‡è§£ç ä¸ºè¶…å‚æ•°"""
            params = {
                "n_estimators": int(np.clip(vec[0], *search_space["n_estimators"])),
                "max_depth": int(np.clip(vec[1], *search_space["max_depth"])),
                "min_samples_split": int(np.clip(vec[2], *search_space["min_samples_split"])),
                "min_samples_leaf": int(np.clip(vec[3], *search_space["min_samples_leaf"])),
                "random_state": 42,
                "n_jobs": -1
            }
            # ç¡®ä¿å‚æ•°åˆç†æ€§
            params["min_samples_split"] = max(params["min_samples_split"], 
                                            params["min_samples_leaf"] + 1)
            return params
        
        def evaluate_particle(vec):
            """è¯„ä¼°ç²’å­æ€§èƒ½"""
            try:
                params = decode_particle(vec)
                model = RandomForestClassifier(**params)
                
                # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°
                cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, 
                                          cv=3, scoring='f1_macro', n_jobs=-1)
                return np.mean(cv_scores)
            except:
                return 0.0
        
        # PSOå‚æ•°
        n_dim = 4
        w = 0.72  # æƒ¯æ€§æƒé‡
        c1 = 1.49  # ä¸ªä½“å­¦ä¹ å› å­
        c2 = 1.49  # ç¾¤ä½“å­¦ä¹ å› å­
        
        # åˆå§‹åŒ–ç²’å­ç¾¤
        lows = np.array([search_space["n_estimators"][0], search_space["max_depth"][0],
                        search_space["min_samples_split"][0], search_space["min_samples_leaf"][0]])
        highs = np.array([search_space["n_estimators"][1], search_space["max_depth"][1],
                         search_space["min_samples_split"][1], search_space["min_samples_leaf"][1]])
        
        positions = np.random.uniform(lows, highs, size=(n_particles, n_dim))
        velocities = np.random.uniform(-np.abs(highs-lows), np.abs(highs-lows), 
                                     size=(n_particles, n_dim)) * 0.1
        
        # è¯„ä¼°åˆå§‹ç²’å­
        pbest_pos = positions.copy()
        pbest_val = np.array([evaluate_particle(p) for p in positions])
        
        gbest_idx = np.argmax(pbest_val)
        gbest_pos = pbest_pos[gbest_idx].copy()
        gbest_val = pbest_val[gbest_idx]
        
        print(f"PSOåˆå§‹æœ€ä¼˜F1-macro: {gbest_val:.4f}")
        
        # PSOè¿­ä»£
        for iteration in range(n_iterations):
            # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
            r1 = np.random.random((n_particles, n_dim))
            r2 = np.random.random((n_particles, n_dim))
            
            velocities = (w * velocities + 
                         c1 * r1 * (pbest_pos - positions) + 
                         c2 * r2 * (gbest_pos - positions))
            
            positions = positions + velocities
            positions = np.clip(positions, lows, highs)
            
            # è¯„ä¼°æ–°ä½ç½®
            vals = np.array([evaluate_particle(p) for p in positions])
            
            # æ›´æ–°ä¸ªä½“æœ€ä¼˜
            improved = vals > pbest_val
            pbest_pos[improved] = positions[improved]
            pbest_val[improved] = vals[improved]
            
            # æ›´æ–°å…¨å±€æœ€ä¼˜
            if pbest_val.max() > gbest_val:
                gbest_idx = np.argmax(pbest_val)
                gbest_pos = pbest_pos[gbest_idx].copy()
                gbest_val = pbest_val[gbest_idx]
            
            if (iteration + 1) % 5 == 0:
                print(f"PSOè¿­ä»£ {iteration + 1}/{n_iterations}, å½“å‰æœ€ä¼˜F1-macro: {gbest_val:.4f}")
        
        # ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        best_params = decode_particle(gbest_pos)
        print(f"\nPSOä¼˜åŒ–å®Œæˆï¼Œæœ€ä¼˜å‚æ•°: {best_params}")
        
        pso_model = RandomForestClassifier(**best_params)
        pso_model.fit(self.X_train_scaled, self.y_train)
        
        # è¯„ä¼°PSOä¼˜åŒ–æ¨¡å‹
        y_pred = pso_model.predict(self.X_test_scaled)
        y_pred_proba = pso_model.predict_proba(self.X_test_scaled)
        
        accuracy = accuracy_score(self.y_test, y_pred)
        f1_macro = f1_score(self.y_test, y_pred, average='macro')
        f1_weighted = f1_score(self.y_test, y_pred, average='weighted')
        
        # ä¿å­˜PSOä¼˜åŒ–æ¨¡å‹
        self.models['RF_PSO_Optimized'] = pso_model
        self.model_scores['RF_PSO_Optimized'] = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'best_params': best_params
        }
        
        print(f"PSOä¼˜åŒ–æ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  F1-macro: {f1_macro:.4f}")
        print(f"  F1-weighted: {f1_weighted:.4f}")
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if f1_macro > max([score['f1_macro'] for score in self.model_scores.values() 
                          if 'f1_macro' in score]):
            self.best_model = pso_model
            print("PSOä¼˜åŒ–æ¨¡å‹æˆä¸ºæ–°çš„æœ€ä½³æ¨¡å‹ï¼")
        
        return True

    def create_optimized_stacking_model(self):
        """ç›´æ¥ä½¿ç”¨PSOä¼˜åŒ–åçš„æœ€ä¼˜å‚æ•°åˆ›å»ºStackingé›†æˆæ¨¡å‹"""
        print("\n=== ä½¿ç”¨æœ€ä¼˜å‚æ•°åˆ›å»ºStackingæ¨¡å‹ ===")
        
        # PSOä¼˜åŒ–åçš„æœ€ä¼˜å‚æ•°
        best_params = {
            'rf_n_estimators': 173,
            'rf_max_depth': 21,
            'rf_min_samples_split': 2,
            'gb_n_estimators': 145,
            'gb_learning_rate': 0.03507181795505348,
            'gb_max_depth': 6,
            'xgb_n_estimators': 197,
            'xgb_learning_rate': 0.030118237660619412,
            'xgb_max_depth': 3
        }
        
        print(f"ä½¿ç”¨æœ€ä¼˜å‚æ•°: {best_params}")
        
        # åˆ›å»ºæœ€ä¼˜Stackingæ¨¡å‹
        rf_optimal = RandomForestClassifier(
            n_estimators=best_params['rf_n_estimators'],
            max_depth=best_params['rf_max_depth'],
            min_samples_split=best_params['rf_min_samples_split'],
            random_state=42,
            n_jobs=-1
        )
        
        gb_optimal = GradientBoostingClassifier(
            n_estimators=best_params['gb_n_estimators'],
            learning_rate=best_params['gb_learning_rate'],
            max_depth=best_params['gb_max_depth'],
            random_state=42
        )
        
        xgb_optimal = xgb.XGBClassifier(
            n_estimators=best_params['xgb_n_estimators'],
            learning_rate=best_params['xgb_learning_rate'],
            max_depth=best_params['xgb_max_depth'],
            random_state=42,
            n_jobs=-1,
            eval_metric='mlogloss'
        )
        
        stacking_optimal = StackingClassifier(
            estimators=[
                ('rf', rf_optimal),
                ('gb', gb_optimal),
                ('xgb', xgb_optimal)
            ],
            final_estimator=LogisticRegression(random_state=42, max_iter=1000),
            cv=5,
            stack_method='predict_proba',
            n_jobs=-1
        )
        
        # è®­ç»ƒæœ€ä¼˜æ¨¡å‹
        print("æ­£åœ¨è®­ç»ƒä¼˜åŒ–åçš„Stackingæ¨¡å‹...")
        stacking_optimal.fit(self.X_train_scaled, self.y_train)
        
        # è¯„ä¼°ä¼˜åŒ–æ¨¡å‹
        y_pred = stacking_optimal.predict(self.X_test_scaled)
        y_pred_proba = stacking_optimal.predict_proba(self.X_test_scaled)
        
        accuracy = accuracy_score(self.y_test, y_pred)
        f1_macro = f1_score(self.y_test, y_pred, average='macro')
        f1_weighted = f1_score(self.y_test, y_pred, average='weighted')
        precision = precision_score(self.y_test, y_pred, average='macro')
        recall = recall_score(self.y_test, y_pred, average='macro')
        
        # ä¿å­˜ä¼˜åŒ–çš„Stackingæ¨¡å‹
        self.models['Stacking_Optimized'] = stacking_optimal
        self.model_scores['Stacking_Optimized'] = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision': precision,
            'recall': recall,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'best_params': best_params
        }
        
        print(f"\nä¼˜åŒ–Stackingæ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  F1-macro: {f1_macro:.4f}")
        print(f"  F1-weighted: {f1_weighted:.4f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"  å¬å›ç‡: {recall:.4f}")
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if f1_macro > max([score['f1_macro'] for score in self.model_scores.values() 
                          if 'f1_macro' in score]):
            self.best_model = stacking_optimal
            print("ä¼˜åŒ–Stackingæ¨¡å‹æˆä¸ºæ–°çš„æœ€ä½³æ¨¡å‹ï¼")
        
        return True

    def optimize_stacking_model_with_pso(self, n_particles=6, n_iterations=2):
        """ä½¿ç”¨PSOä¼˜åŒ–Stackingé›†æˆæ¨¡å‹çš„è¶…å‚æ•°"""
        print("\n=== å¼€å§‹Stackingæ¨¡å‹PSOè¶…å‚æ•°ä¼˜åŒ– ===")
        
        # å®šä¹‰æœç´¢ç©ºé—´ï¼ˆé’ˆå¯¹åŸºç¡€æ¨¡å‹çš„å…³é”®è¶…å‚æ•°ï¼‰
        search_space = {
            # éšæœºæ£®æ—å‚æ•°
            "rf_n_estimators": (50, 200),
            "rf_max_depth": (5, 30),
            "rf_min_samples_split": (2,5),
            # æ¢¯åº¦æå‡å‚æ•°
            "gb_n_estimators": (50, 200),
            "gb_learning_rate": (0.01, 0.10 ),
            "gb_max_depth": (3, 8),
            # XGBoostå‚æ•°
            "xgb_n_estimators": (50, 200),
            "xgb_learning_rate": (0.01, 0.1),
            "xgb_max_depth": (3, 10)
        }
        
        def decode_particle(particle):
            """å°†ç²’å­ä½ç½®è§£ç ä¸ºæ¨¡å‹å‚æ•°"""
            return {
                'rf_n_estimators': int(particle[0]),
                'rf_max_depth': int(particle[1]),
                'rf_min_samples_split': int(particle[2]),
                'gb_n_estimators': int(particle[3]),
                'gb_learning_rate': particle[4],
                'gb_max_depth': int(particle[5]),
                'xgb_n_estimators': int(particle[6]),
                'xgb_learning_rate': particle[7],
                'xgb_max_depth': int(particle[8])
            }
        
        def evaluate_particle(particle):
            """è¯„ä¼°ç²’å­ï¼ˆè¿”å›F1-macroå¾—åˆ†ï¼‰"""
            try:
                params = decode_particle(particle)
                
                # åˆ›å»ºåŸºç¡€æ¨¡å‹
                rf_model = RandomForestClassifier(
                    n_estimators=params['rf_n_estimators'],
                    max_depth=params['rf_max_depth'],
                    min_samples_split=params['rf_min_samples_split'],
                    random_state=42,
                    n_jobs=-1
                )
                
                gb_model = GradientBoostingClassifier(
                    n_estimators=params['gb_n_estimators'],
                    learning_rate=params['gb_learning_rate'],
                    max_depth=params['gb_max_depth'],
                    random_state=42
                )
                
                xgb_model = xgb.XGBClassifier(
                    n_estimators=params['xgb_n_estimators'],
                    learning_rate=params['xgb_learning_rate'],
                    max_depth=params['xgb_max_depth'],
                    random_state=42,
                    n_jobs=-1,
                    eval_metric='mlogloss'
                )
                
                # åˆ›å»ºStackingæ¨¡å‹
                stacking_model = StackingClassifier(
                    estimators=[
                        ('rf', rf_model),
                        ('gb', gb_model),
                        ('xgb', xgb_model)
                    ],
                    final_estimator=LogisticRegression(random_state=42, max_iter=1000),
                    cv=3,  # å‡å°‘CVæŠ˜æ•°ä»¥åŠ é€Ÿ
                    stack_method='predict_proba',
                    n_jobs=-1
                )
                
                # è®­ç»ƒå’Œè¯„ä¼°
                stacking_model.fit(self.X_train_scaled, self.y_train)
                y_pred = stacking_model.predict(self.X_test_scaled)
                f1_macro = f1_score(self.y_test, y_pred, average='macro')
                
                return f1_macro
                
            except Exception as e:
                print(f"ç²’å­è¯„ä¼°å¤±è´¥: {e}")
                return 0.0
        
        # PSOå‚æ•°
        n_dim = 9  # 9ä¸ªè¶…å‚æ•°
        w = 0.72  # æƒ¯æ€§æƒé‡
        c1 = 1.49  # ä¸ªä½“å­¦ä¹ å› å­
        c2 = 1.49  # ç¾¤ä½“å­¦ä¹ å› å­
        
        # åˆå§‹åŒ–ç²’å­ç¾¤
        lows = np.array([
            search_space["rf_n_estimators"][0], search_space["rf_max_depth"][0], search_space["rf_min_samples_split"][0],
            search_space["gb_n_estimators"][0], search_space["gb_learning_rate"][0], search_space["gb_max_depth"][0],
            search_space["xgb_n_estimators"][0], search_space["xgb_learning_rate"][0], search_space["xgb_max_depth"][0]
        ])
        highs = np.array([
            search_space["rf_n_estimators"][1], search_space["rf_max_depth"][1], search_space["rf_min_samples_split"][1],
            search_space["gb_n_estimators"][1], search_space["gb_learning_rate"][1], search_space["gb_max_depth"][1],
            search_space["xgb_n_estimators"][1], search_space["xgb_learning_rate"][1], search_space["xgb_max_depth"][1]
        ])
        
        positions = np.random.uniform(lows, highs, size=(n_particles, n_dim))
        velocities = np.random.uniform(-np.abs(highs-lows), np.abs(highs-lows), 
                                     size=(n_particles, n_dim)) * 0.1
        
        # è¯„ä¼°åˆå§‹ç²’å­
        print("è¯„ä¼°åˆå§‹ç²’å­ç¾¤...")
        pbest_pos = positions.copy()
        pbest_val = np.array([evaluate_particle(p) for p in positions])
        
        gbest_idx = np.argmax(pbest_val)
        gbest_pos = pbest_pos[gbest_idx].copy()
        gbest_val = pbest_val[gbest_idx]
        
        print(f"PSOåˆå§‹æœ€ä¼˜F1-macro: {gbest_val:.4f}")
        
        # PSOè¿­ä»£
        for iteration in range(n_iterations):
            print(f"PSOè¿­ä»£ {iteration + 1}/{n_iterations}...")
            
            # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
            r1 = np.random.random((n_particles, n_dim))
            r2 = np.random.random((n_particles, n_dim))
            
            velocities = (w * velocities + 
                         c1 * r1 * (pbest_pos - positions) + 
                         c2 * r2 * (gbest_pos - positions))
            
            positions = positions + velocities
            positions = np.clip(positions, lows, highs)
            
            # è¯„ä¼°æ–°ä½ç½®
            vals = np.array([evaluate_particle(p) for p in positions])
            
            # æ›´æ–°ä¸ªä½“æœ€ä¼˜
            improved = vals > pbest_val
            pbest_pos[improved] = positions[improved]
            pbest_val[improved] = vals[improved]
            
            # æ›´æ–°å…¨å±€æœ€ä¼˜
            if pbest_val.max() > gbest_val:
                gbest_idx = np.argmax(pbest_val)
                gbest_pos = pbest_pos[gbest_idx].copy()
                gbest_val = pbest_val[gbest_idx]
            
            if (iteration + 1) % 5 == 0:
                print(f"  å½“å‰æœ€ä¼˜F1-macro: {gbest_val:.4f}")
        
        # ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        best_params = decode_particle(gbest_pos)
        print(f"\nPSOä¼˜åŒ–å®Œæˆï¼Œæœ€ä¼˜å‚æ•°: {best_params}")
        
        # åˆ›å»ºæœ€ä¼˜Stackingæ¨¡å‹
        rf_optimal = RandomForestClassifier(
            n_estimators=best_params['rf_n_estimators'],
            max_depth=best_params['rf_max_depth'],
            min_samples_split=best_params['rf_min_samples_split'],
            random_state=42,
            n_jobs=-1
        )
        
        gb_optimal = GradientBoostingClassifier(
            n_estimators=best_params['gb_n_estimators'],
            learning_rate=best_params['gb_learning_rate'],
            max_depth=best_params['gb_max_depth'],
            random_state=42
        )
        
        xgb_optimal = xgb.XGBClassifier(
            n_estimators=best_params['xgb_n_estimators'],
            learning_rate=best_params['xgb_learning_rate'],
            max_depth=best_params['xgb_max_depth'],
            random_state=42,
            n_jobs=-1,
            eval_metric='mlogloss'
        )
        
        stacking_optimal = StackingClassifier(
            estimators=[
                ('rf', rf_optimal),
                ('gb', gb_optimal),
                ('xgb', xgb_optimal)
            ],
            final_estimator=LogisticRegression(random_state=42, max_iter=1000),
            cv=5,
            stack_method='predict_proba',
            n_jobs=-1
        )
        
        # è®­ç»ƒæœ€ä¼˜æ¨¡å‹
        stacking_optimal.fit(self.X_train_scaled, self.y_train)
        
        # è¯„ä¼°PSOä¼˜åŒ–æ¨¡å‹
        y_pred = stacking_optimal.predict(self.X_test_scaled)
        y_pred_proba = stacking_optimal.predict_proba(self.X_test_scaled)
        
        accuracy = accuracy_score(self.y_test, y_pred)
        f1_macro = f1_score(self.y_test, y_pred, average='macro')
        f1_weighted = f1_score(self.y_test, y_pred, average='weighted')
        precision = precision_score(self.y_test, y_pred, average='macro')
        recall = recall_score(self.y_test, y_pred, average='macro')
        
        # ä¿å­˜PSOä¼˜åŒ–çš„Stackingæ¨¡å‹
        self.models['Stacking_PSO_Optimized'] = stacking_optimal
        self.model_scores['Stacking_PSO_Optimized'] = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision': precision,
            'recall': recall,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'best_params': best_params
        }
        
        print(f"\nPSOä¼˜åŒ–Stackingæ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  F1-macro: {f1_macro:.4f}")
        print(f"  F1-weighted: {f1_weighted:.4f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"  å¬å›ç‡: {recall:.4f}")
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if f1_macro > max([score['f1_macro'] for score in self.model_scores.values() 
                          if 'f1_macro' in score]):
            self.best_model = stacking_optimal
            print("PSOä¼˜åŒ–Stackingæ¨¡å‹æˆä¸ºæ–°çš„æœ€ä½³æ¨¡å‹ï¼")
        
        return True

    def create_weighted_probability_fusion(self):
        """åˆ›å»ºé¢„æµ‹æ¦‚ç‡åŠ æƒèåˆæ¨¡å‹"""
        print("\n=== åˆ›å»ºé¢„æµ‹æ¦‚ç‡åŠ æƒèåˆæ¨¡å‹ ===")
        
        # ç¡®ä¿åŸºç¡€æ¨¡å‹å­˜åœ¨
        required_models = ['Random_Forest', 'Gradient_Boosting', 'XGBoost']
        available_models = []
        
        for model_name in required_models:
            if model_name in self.models:
                available_models.append(model_name)
            else:
                print(f"âš ï¸ æ¨¡å‹ {model_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        if len(available_models) < 2:
            print("âŒ å¯ç”¨æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡ŒåŠ æƒèåˆ")
            return False
        
        print(f"ä½¿ç”¨æ¨¡å‹è¿›è¡ŒåŠ æƒèåˆ: {', '.join(available_models)}")
        
        # è·å–å„æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
        model_probabilities = {}
        model_weights = {}
        
        for model_name in available_models:
            model = self.models[model_name]
            y_pred_proba = model.predict_proba(self.X_test_scaled)
            model_probabilities[model_name] = y_pred_proba
            
            # åŸºäºF1-macroå¾—åˆ†è®¡ç®—æƒé‡
            f1_score_val = self.model_scores[model_name]['f1_macro']
            model_weights[model_name] = f1_score_val
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(model_weights.values())
        normalized_weights = {name: weight/total_weight for name, weight in model_weights.items()}
        
        print("æ¨¡å‹æƒé‡åˆ†é…:")
        for name, weight in normalized_weights.items():
            print(f"  {name}: {weight:.4f}")
        
        # è®¡ç®—åŠ æƒå¹³å‡æ¦‚ç‡
        weighted_proba = np.zeros_like(list(model_probabilities.values())[0])
        
        for model_name, proba in model_probabilities.items():
            weight = normalized_weights[model_name]
            weighted_proba += weight * proba
        
        # ç”Ÿæˆæœ€ç»ˆé¢„æµ‹
        y_pred_weighted = np.argmax(weighted_proba, axis=1)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(self.y_test, y_pred_weighted)
        f1_macro = f1_score(self.y_test, y_pred_weighted, average='macro')
        f1_weighted = f1_score(self.y_test, y_pred_weighted, average='weighted')
        precision = precision_score(self.y_test, y_pred_weighted, average='macro')
        recall = recall_score(self.y_test, y_pred_weighted, average='macro')
        
        # ä¿å­˜åŠ æƒèåˆæ¨¡å‹ç»“æœ
        fusion_name = 'Weighted_Probability_Fusion'
        self.model_scores[fusion_name] = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision': precision,
            'recall': recall,
            'predictions': y_pred_weighted,
            'probabilities': weighted_proba,
            'model_weights': normalized_weights,
            'used_models': available_models
        }
        
        print(f"\né¢„æµ‹æ¦‚ç‡åŠ æƒèåˆæ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  F1-macro: {f1_macro:.4f}")
        print(f"  F1-weighted: {f1_weighted:.4f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"  å¬å›ç‡: {recall:.4f}")
        
        # ä¸å•ä¸ªæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
        print(f"\nä¸å•ä¸ªæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
        for model_name in available_models:
            single_f1 = self.model_scores[model_name]['f1_macro']
            improvement = f1_macro - single_f1
            print(f"  vs {model_name}: {improvement:+.4f}")
        
        return True

    def create_feature_level_fusion(self):
        """åˆ›å»ºç‰¹å¾çº§èåˆæ¨¡å‹"""
        print("\n=== åˆ›å»ºç‰¹å¾çº§èåˆæ¨¡å‹ ===")
        
        # ç¡®ä¿åŸºç¡€æ¨¡å‹å­˜åœ¨
        required_models = ['Random_Forest', 'Gradient_Boosting', 'XGBoost']
        available_models = []
        
        for model_name in required_models:
            if model_name in self.models:
                available_models.append(model_name)
            else:
                print(f"âš ï¸ æ¨¡å‹ {model_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        if len(available_models) < 2:
            print("âŒ å¯ç”¨æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç‰¹å¾çº§èåˆ")
            return False
        
        print(f"ä½¿ç”¨æ¨¡å‹è¿›è¡Œç‰¹å¾çº§èåˆ: {', '.join(available_models)}")
        
        # æå–å„æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        feature_importances = {}
        
        for model_name in available_models:
            model = self.models[model_name]
            if hasattr(model, 'feature_importances_'):
                feature_importances[model_name] = model.feature_importances_
            else:
                print(f"âš ï¸ æ¨¡å‹ {model_name} ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§ï¼Œè·³è¿‡")
                continue
        
        if len(feature_importances) < 2:
            print("âŒ æ”¯æŒç‰¹å¾é‡è¦æ€§çš„æ¨¡å‹æ•°é‡ä¸è¶³")
            return False
        
        # è®¡ç®—åŠ æƒç‰¹å¾é‡è¦æ€§
        model_weights = {}
        for model_name in feature_importances.keys():
            f1_score_val = self.model_scores[model_name]['f1_macro']
            model_weights[model_name] = f1_score_val
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(model_weights.values())
        normalized_weights = {name: weight/total_weight for name, weight in model_weights.items()}
        
        # è®¡ç®—èåˆç‰¹å¾é‡è¦æ€§
        n_features = len(list(feature_importances.values())[0])
        fused_importance = np.zeros(n_features)
        
        for model_name, importance in feature_importances.items():
            weight = normalized_weights[model_name]
            fused_importance += weight * importance
        
        # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼ˆå‰50%ï¼‰
        n_selected = max(1, n_features // 2)
        selected_indices = np.argsort(fused_importance)[-n_selected:]
        
        print(f"é€‰æ‹©äº† {n_selected} ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼ˆå…± {n_features} ä¸ªï¼‰")
        
        # ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾è®­ç»ƒæ–°æ¨¡å‹
        X_train_selected = self.X_train_scaled[:, selected_indices]
        X_test_selected = self.X_test_scaled[:, selected_indices]
        
        # åˆ›å»ºåŸºäºç‰¹å¾é€‰æ‹©çš„éšæœºæ£®æ—æ¨¡å‹
        feature_fusion_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # è®­ç»ƒæ¨¡å‹
        feature_fusion_model.fit(X_train_selected, self.y_train)
        
        # é¢„æµ‹
        y_pred = feature_fusion_model.predict(X_test_selected)
        y_pred_proba = feature_fusion_model.predict_proba(X_test_selected)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(self.y_test, y_pred)
        f1_macro = f1_score(self.y_test, y_pred, average='macro')
        f1_weighted = f1_score(self.y_test, y_pred, average='weighted')
        precision = precision_score(self.y_test, y_pred, average='macro')
        recall = recall_score(self.y_test, y_pred, average='macro')
        
        # ä¿å­˜ç‰¹å¾çº§èåˆæ¨¡å‹
        fusion_name = 'Feature_Level_Fusion'
        self.models[fusion_name] = feature_fusion_model
        self.model_scores[fusion_name] = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'precision': precision,
            'recall': recall,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'selected_features': selected_indices,
            'feature_importance': fused_importance,
            'n_selected_features': n_selected,
            'model_weights': normalized_weights
        }
        
        print(f"\nç‰¹å¾çº§èåˆæ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  F1-macro: {f1_macro:.4f}")
        print(f"  F1-weighted: {f1_weighted:.4f}")
        print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"  å¬å›ç‡: {recall:.4f}")
        
        return True

    # def feature_selection_analysis(self, n_features=30):
    #     """ç‰¹å¾é€‰æ‹©åˆ†æ"""
    #     print(f"\n=== ç‰¹å¾é€‰æ‹©åˆ†æï¼ˆé€‰æ‹©å‰{n_features}ä¸ªç‰¹å¾ï¼‰===")
        
    #     # ä½¿ç”¨RFEè¿›è¡Œç‰¹å¾é€‰æ‹©
    #     rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)
    #     rfe = RFE(estimator=rf_selector, n_features_to_select=n_features, step=1)
    #     rfe.fit(self.X_train_scaled, self.y_train)
        
    #     # è·å–é€‰æ‹©çš„ç‰¹å¾
    #     selected_features_mask = rfe.support_
    #     selected_feature_names = [self.selected_features[i] for i in range(len(selected_features_mask)) 
    #                             if selected_features_mask[i]]
        
    #     print(f"RFEé€‰æ‹©çš„å‰{n_features}ä¸ªç‰¹å¾:")
    #     for i, feature in enumerate(selected_feature_names, 1):
    #         print(f"  {i:2d}. {feature}")
        
    #     # ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾è®­ç»ƒæ¨¡å‹
    #     X_train_selected = self.X_train_scaled[:, selected_features_mask]
    #     X_test_selected = self.X_test_scaled[:, selected_features_mask]
        
    #     rf_selected = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
    #     rf_selected.fit(X_train_selected, self.y_train)
        
    #     y_pred_selected = rf_selected.predict(X_test_selected)
        
    #     accuracy_selected = accuracy_score(self.y_test, y_pred_selected)
    #     f1_macro_selected = f1_score(self.y_test, y_pred_selected, average='macro')
        
    #     print(f"\nç‰¹å¾é€‰æ‹©åæ¨¡å‹æ€§èƒ½:")
    #     print(f"  ä½¿ç”¨ç‰¹å¾æ•°: {n_features}")
    #     print(f"  å‡†ç¡®ç‡: {accuracy_selected:.4f}")
    #     print(f"  F1-macro: {f1_macro_selected:.4f}")
        
    #     # ä¿å­˜ç‰¹å¾é€‰æ‹©æ¨¡å‹
    #     self.models['RF_Feature_Selected'] = rf_selected
    #     self.model_scores['RF_Feature_Selected'] = {
    #         'accuracy': accuracy_selected,
    #         'f1_macro': f1_macro_selected,
    #         'predictions': y_pred_selected,
    #         'selected_features': selected_feature_names,
    #         'feature_mask': selected_features_mask
    #     }
        
    #     return selected_feature_names

    def plot_model_comparison(self):
        """ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾"""
        print("\n=== ç”Ÿæˆæ¨¡å‹æ¯”è¾ƒå¯è§†åŒ– ===")
        
        # å‡†å¤‡æ•°æ®
        model_names = list(self.model_scores.keys())
        accuracies = [self.model_scores[name]['accuracy'] for name in model_names]
        f1_macros = [self.model_scores[name]['f1_macro'] for name in model_names]
        
        # åˆ›å»ºæ¯”è¾ƒå›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å‡†ç¡®ç‡æ¯”è¾ƒ
        bars1 = ax1.bar(model_names, accuracies, color='skyblue', alpha=0.7)
        ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ', fontsize=14, fontweight='bold')
        ax1.set_ylabel('å‡†ç¡®ç‡', fontsize=12)
        ax1.set_ylim(0, 1)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars1, accuracies):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # F1-macroæ¯”è¾ƒ
        bars2 = ax2.bar(model_names, f1_macros, color='lightcoral', alpha=0.7)
        ax2.set_title('æ¨¡å‹F1-Macroå¾—åˆ†æ¯”è¾ƒ', fontsize=14, fontweight='bold')
        ax2.set_ylabel('F1-Macroå¾—åˆ†', fontsize=12)
        ax2.set_ylim(0, 1)
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, f1 in zip(bars2, f1_macros):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # æ—‹è½¬xè½´æ ‡ç­¾
        for ax in [ax1, ax2]:
            ax.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/model_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()

    def plot_confusion_matrices(self):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        print("\n=== ç”Ÿæˆæ··æ·†çŸ©é˜µ ===")
        
        n_models = len(self.models)
        fig, axes = plt.subplots(2, (n_models + 1) // 2, figsize=(15, 10))
        if n_models == 1:
            axes = [axes]
        axes = axes.flatten()
        
        class_names = self.label_encoder.classes_
        
        for idx, (model_name, model) in enumerate(self.models.items()):
            y_pred = self.model_scores[model_name]['predictions']
            cm = confusion_matrix(self.y_test, y_pred)
            
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=class_names, yticklabels=class_names,
                       ax=axes[idx])
            axes[idx].set_title(f'{model_name} æ··æ·†çŸ©é˜µ')
            axes[idx].set_xlabel('é¢„æµ‹æ ‡ç­¾')
            axes[idx].set_ylabel('çœŸå®æ ‡ç­¾')
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_models, len(axes)):
            axes[idx].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')
        plt.show()

    def plot_roc_curves(self):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        print("\n=== ç”ŸæˆROCæ›²çº¿ ===")
        
        class_names = self.label_encoder.classes_
        n_classes = len(class_names)
        
        # äºŒå€¼åŒ–æ ‡ç­¾
        y_test_bin = label_binarize(self.y_test, classes=range(n_classes))
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for class_idx in range(min(n_classes, 4)):  # æœ€å¤šæ˜¾ç¤º4ä¸ªç±»åˆ«
            ax = axes[class_idx]
            
            for model_idx, (model_name, model) in enumerate(self.models.items()):
                if 'probabilities' in self.model_scores[model_name]:
                    y_proba = self.model_scores[model_name]['probabilities']
                    
                    if class_idx < y_proba.shape[1]:
                        fpr, tpr, _ = roc_curve(y_test_bin[:, class_idx], y_proba[:, class_idx])
                        roc_auc = auc(fpr, tpr)
                        
                        ax.plot(fpr, tpr, color=colors[model_idx % len(colors)],
                               label=f'{model_name} (AUC = {roc_auc:.3f})', linewidth=2)
            
            ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)
            ax.set_xlabel('å‡é˜³æ€§ç‡ (FPR)')
            ax.set_ylabel('çœŸé˜³æ€§ç‡ (TPR)')
            ax.set_title(f'ROCæ›²çº¿ - {class_names[class_idx]}')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/roc_curves.png', dpi=300, bbox_inches='tight')
        plt.show()

    def plot_feature_importance(self):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        print("\n=== ç”Ÿæˆç‰¹å¾é‡è¦æ€§åˆ†æ ===")
        
        if self.best_model is None:
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼")
            return
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
            feature_names = self.selected_features
            
            # æ’åº
            indices = np.argsort(importances)[::-1]
            
            # é€‰æ‹©å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
            top_n = min(20, len(importances))
            top_indices = indices[:top_n]
            top_importances = importances[top_indices]
            top_features = [feature_names[i] for i in top_indices]
            
            # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
            plt.figure(figsize=(12, 8))
            bars = plt.barh(range(top_n), top_importances[::-1], color='lightgreen', alpha=0.7)
            plt.yticks(range(top_n), [top_features[i] for i in range(top_n-1, -1, -1)])
            plt.xlabel('ç‰¹å¾é‡è¦æ€§')
            plt.title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå‰20ä¸ªç‰¹å¾ï¼‰', fontsize=14, fontweight='bold')
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, bar in enumerate(bars):
                plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,
                        f'{top_importances[top_n-1-i]:.3f}', 
                        va='center', ha='left', fontsize=9)
            
            plt.tight_layout()
            plt.savefig(f'{self.results_dir}/feature_importance_analysis.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            # ä¿å­˜ç‰¹å¾é‡è¦æ€§åˆ°CSV
            importance_df = pd.DataFrame({
                'feature': top_features,
                'importance': top_importances
            })
            importance_df.to_csv(f'{self.results_dir}/feature_importance.csv', index=False)
            print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {self.results_dir}/feature_importance.csv")

    def generate_classification_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š ===")
        
        class_names = self.label_encoder.classes_
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹ç”ŸæˆæŠ¥å‘Š
        all_reports = {}
        
        for model_name in self.models.keys():
            y_pred = self.model_scores[model_name]['predictions']
            
            # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
            report = classification_report(self.y_test, y_pred, 
                                         target_names=class_names, 
                                         output_dict=True)
            all_reports[model_name] = report
            
            print(f"\n{model_name} åˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(self.y_test, y_pred, target_names=class_names))
        
        # ä¿å­˜æŠ¥å‘Šåˆ°CSV
        report_data = []
        for model_name, report in all_reports.items():
            for class_name in class_names:
                if class_name in report:
                    report_data.append({
                        'model': model_name,
                        'class': class_name,
                        'precision': report[class_name]['precision'],
                        'recall': report[class_name]['recall'],
                        'f1-score': report[class_name]['f1-score'],
                        'support': report[class_name]['support']
                    })
            
            # æ·»åŠ æ€»ä½“æŒ‡æ ‡
            report_data.append({
                'model': model_name,
                'class': 'macro avg',
                'precision': report['macro avg']['precision'],
                'recall': report['macro avg']['recall'],
                'f1-score': report['macro avg']['f1-score'],
                'support': report['macro avg']['support']
            })
        
        report_df = pd.DataFrame(report_data)
        report_df.to_csv(f'{self.results_dir}/classification_report.csv', index=False)
        print(f"åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ° {self.results_dir}/classification_report.csv")

    def save_all_models(self):
        """ä¿å­˜å…¨éƒ¨è®­ç»ƒçš„æ¨¡å‹"""
        if not self.model_scores:
            print("æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹ï¼")
            return
        
        # æŒ‰F1-macroåˆ†æ•°æ’åºï¼Œè·å–å…¨éƒ¨æ¨¡å‹
        sorted_models = sorted(self.model_scores.items(), 
                             key=lambda x: x[1]['f1_macro'], 
                             reverse=True)
        
        print(f"\nä¿å­˜å…¨éƒ¨{len(sorted_models)}ä¸ªè®­ç»ƒçš„æ¨¡å‹:")
        
        for i, (model_name, scores) in enumerate(sorted_models, 1):
            # è·å–å¯¹åº”çš„æ¨¡å‹å¯¹è±¡
            if model_name in self.models:
                model_obj = self.models[model_name]
                
                # æ„å»ºæ–‡ä»¶åï¼Œä½¿ç”¨æ’åå’Œæ¨¡å‹åç§°
                model_path = f'{self.results_dir}/rank_{i}_{model_name}_model.pkl'
                
                # ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯
                model_info = {
                    'model': model_obj,
                    'model_name': model_name,
                    'rank': i,
                    'scaler': self.scaler,
                    'label_encoder': self.label_encoder,
                    'selected_features': self.selected_features,
                    'scores': scores,
                    'f1_macro': scores['f1_macro'],
                    'accuracy': scores['accuracy'],
                    'precision': scores['precision'],
                    'recall': scores['recall']
                }
                
                with open(model_path, 'wb') as f:
                    pickle.dump(model_info, f)
                
                print(f"  ç¬¬{i}å: {model_name} (F1-macro: {scores['f1_macro']:.4f}) -> {model_path}")
            else:
                print(f"  è­¦å‘Š: æ¨¡å‹ {model_name} ä¸å­˜åœ¨äºself.modelsä¸­ï¼Œè·³è¿‡ä¿å­˜")
        
        # ä¿å­˜ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ¨¡å‹çš„æ±‡æ€»æ–‡ä»¶
        all_models_summary_path = f'{self.results_dir}/all_models_summary.pkl'
        summary_info = {
            'all_models_ranked': sorted_models,
            'total_models': len(sorted_models),
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'selected_features': self.selected_features,
            'all_model_scores': self.model_scores,
            'best_model_name': sorted_models[0][0] if sorted_models else None,
            'best_f1_score': sorted_models[0][1]['f1_macro'] if sorted_models else None
        }
        
        with open(all_models_summary_path, 'wb') as f:
            pickle.dump(summary_info, f)
        
        print(f"\nå…¨éƒ¨{len(sorted_models)}ä¸ªæ¨¡å‹æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜åˆ°: {all_models_summary_path}")
        print(f"æœ€ä½³æ¨¡å‹: {sorted_models[0][0]} (F1-macro: {sorted_models[0][1]['f1_macro']:.4f})" if sorted_models else "æ— æ¨¡å‹")

    def _print_final_summary(self):
        """æ‰“å°æœ€ç»ˆç»“æœæ€»ç»“"""
        print("\nğŸ“Š æœ€ç»ˆè¯Šæ–­ç»“æœæ€»ç»“:")
        print("-" * 50)
        
        if not self.model_scores:
            print("âŒ æ²¡æœ‰æ¨¡å‹è¯„ä¼°ç»“æœå¯æ˜¾ç¤º")
            return
        
        # æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½
        print(f"ğŸ“ˆ å…±è®­ç»ƒäº† {len(self.model_scores)} ä¸ªæ¨¡å‹:")
        
        # æŒ‰F1-macroåˆ†æ•°æ’åº
        sorted_models = sorted(self.model_scores.items(), 
                             key=lambda x: x[1]['f1_macro'], 
                             reverse=True)
        
        for i, (model_name, scores) in enumerate(sorted_models, 1):
            print(f"  {i}. {model_name}:")
            print(f"     å‡†ç¡®ç‡: {scores['accuracy']:.4f}")
            print(f"     F1-macro: {scores['f1_macro']:.4f}")
            print(f"     ç²¾ç¡®ç‡: {scores['precision']:.4f}")
            print(f"     å¬å›ç‡: {scores['recall']:.4f}")
            if i == 1:
                print("     ğŸ† æœ€ä½³æ¨¡å‹")
            print()
        
        # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
        if hasattr(self, 'best_model') and self.best_model is not None:
            best_model_name = max(self.model_scores.items(), 
                                key=lambda x: x[1]['f1_macro'])[0]
            print(f"ğŸ¯ æœ€ä½³æ¨¡å‹: {best_model_name}")
            print(f"ğŸ¯ æœ€ä½³F1-macroå¾—åˆ†: {max(score['f1_macro'] for score in self.model_scores.values()):.4f}")
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        if hasattr(self, 'X_train') and hasattr(self, 'X_test'):
            print(f"\nğŸ“‹ æ•°æ®é›†ä¿¡æ¯:")
            print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°: {len(self.X_train)}")
            print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(self.X_test)}")
            print(f"   ç‰¹å¾æ•°é‡: {self.X_train.shape[1] if hasattr(self.X_train, 'shape') else 'N/A'}")
        
        # æ˜¾ç¤ºæ•…éšœç±»åˆ«ä¿¡æ¯
        if hasattr(self, 'label_encoder') and self.label_encoder is not None:
            print(f"   æ•…éšœç±»åˆ«æ•°: {len(self.label_encoder.classes_)}")
            print(f"   æ•…éšœç±»åˆ«: {', '.join(self.label_encoder.classes_)}")

    def optimize_selected_models(self):
        """å¯¹å€¼å¾—æ¢ç´¢çš„æ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼ˆåªå¯¹éšæœºæ£®æ—è¿›è¡Œä¼˜åŒ–ï¼‰"""
        print("\n=== å¼€å§‹å¯¹é€‰å®šæ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ– ===")
        
        # åªå¯¹éšæœºæ£®æ—è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        models_to_optimize = ['Random_Forest']
        
        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ï¼ˆè¿ç»­åŒºé—´èŒƒå›´ï¼‰
        param_distributions = {
            'Random_Forest': {
                'n_estimators': randint(50, 300),           # 100-300ä¹‹é—´çš„æ•´æ•°
                'max_depth': randint(5, 30),                # 10-30ä¹‹é—´çš„æ•´æ•°
                'min_samples_split': randint(2, 10),         # 2-10ä¹‹é—´çš„æ•´æ•°
                'min_samples_leaf': randint(1, 5),           # 1-5ä¹‹é—´çš„æ•´æ•°
                'max_features': uniform(0.5, 1.0)           # 0.5-1.0ä¹‹é—´çš„è¿ç»­å€¼
            }
            # ç§»é™¤äº†Gradient_Boostingçš„è¶…å‚æ•°é…ç½®
        }
        
        # å®šä¹‰åŸºç¡€æ¨¡å‹
        base_models = {
            'Random_Forest': RandomForestClassifier(random_state=42, n_jobs=-1)
            # ç§»é™¤äº†Gradient_Boostingçš„åŸºç¡€æ¨¡å‹é…ç½®
        }
        
        optimized_models = {}
        
        print(f"å°†å¯¹ä»¥ä¸‹æ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–: {', '.join(models_to_optimize)}")
        print("å…¶ä»–æ¨¡å‹ï¼ˆSVM_RBF, Logistic_Regression, Gradient_Boostingï¼‰ä¿æŒåŸºç¡€é…ç½®")
        
        # å¯¹é€‰å®šçš„æ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        for model_name in models_to_optimize:
            if model_name in self.models:
                print(f"\nğŸ”§ ä¼˜åŒ– {model_name}...")
                start_time = time.time()
                
                try:
                    # åˆ›å»ºRandomizedSearchCVè¿›è¡ŒåŒºé—´æœç´¢
                    random_search = RandomizedSearchCV(
                        estimator=base_models[model_name],
                        param_distributions=param_distributions[model_name],
                        n_iter=50,  # éšæœºæœç´¢50æ¬¡
                        cv=3,
                        scoring='f1_macro',
                        n_jobs=-1,
                        verbose=0,
                        random_state=42
                    )
                    
                    # æ‰§è¡Œéšæœºæœç´¢
                    random_search.fit(self.X_train_scaled, self.y_train)
                    
                    # è·å–æœ€ä½³æ¨¡å‹
                    best_model = random_search.best_estimator_
                    
                    # é¢„æµ‹
                    y_pred = best_model.predict(self.X_test_scaled)
                    y_pred_proba = best_model.predict_proba(self.X_test_scaled)
                    
                    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                    accuracy = accuracy_score(self.y_test, y_pred)
                    f1_macro = f1_score(self.y_test, y_pred, average='macro')
                    f1_weighted = f1_score(self.y_test, y_pred, average='weighted')
                    precision = precision_score(self.y_test, y_pred, average='macro')
                    recall = recall_score(self.y_test, y_pred, average='macro')
                    optimization_time = time.time() - start_time
                    
                    # ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹
                    optimized_name = f"{model_name}_Optimized"
                    optimized_models[optimized_name] = best_model
                    self.model_scores[optimized_name] = {
                        'accuracy': accuracy,
                        'f1_macro': f1_macro,
                        'f1_weighted': f1_weighted,
                        'precision': precision,
                        'recall': recall,
                        'predictions': y_pred,
                        'probabilities': y_pred_proba,
                        'best_params': random_search.best_params_,
                        'optimization_time': optimization_time,
                        'cv_score': random_search.best_score_
                    }
                    
                    print(f"  âœ… {model_name} ä¼˜åŒ–å®Œæˆ:")
                    print(f"    æœ€ä½³å‚æ•°: {random_search.best_params_}")
                    print(f"    äº¤å‰éªŒè¯å¾—åˆ†: {random_search.best_score_:.4f}")
                    print(f"    æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
                    print(f"    æµ‹è¯•é›†F1-macro: {f1_macro:.4f}")
                    print(f"    ä¼˜åŒ–æ—¶é—´: {optimization_time:.2f}ç§’")
                    
                    # æ¯”è¾ƒä¼˜åŒ–å‰åçš„æ€§èƒ½
                    original_f1 = self.model_scores[model_name]['f1_macro']
                    improvement = f1_macro - original_f1
                    print(f"    æ€§èƒ½æå‡: {improvement:+.4f}")
                    
                except Exception as e:
                    print(f"  âŒ {model_name} ä¼˜åŒ–å¤±è´¥: {str(e)}")
                    continue
            else:
                print(f"âš ï¸ æ¨¡å‹ {model_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¼˜åŒ–")
        
        # ä½¿ç”¨æœ€ä¼˜å‚æ•°ç›´æ¥åˆ›å»ºStackingæ¨¡å‹
        if 'Stacking_RF_GB_XGB' in self.models:
            print(f"\nğŸ”§ ä½¿ç”¨æœ€ä¼˜å‚æ•°åˆ›å»ºStackingé›†æˆæ¨¡å‹...")
            try:
                self.create_optimized_stacking_model()
                print("  âœ… ä¼˜åŒ–Stackingæ¨¡å‹åˆ›å»ºå®Œæˆ")
            except Exception as e:
                print(f"  âŒ ä¼˜åŒ–Stackingæ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
        
        # åˆ›å»ºé¢„æµ‹æ¦‚ç‡åŠ æƒèåˆæ¨¡å‹ï¼ˆå·²æ³¨é‡Šï¼Œä¿ç•™ä»£ç å¤‡ç”¨ï¼‰
        # print(f"\nğŸ”§ åˆ›å»ºé¢„æµ‹æ¦‚ç‡åŠ æƒèåˆæ¨¡å‹...")
        # try:
        #     self.create_weighted_probability_fusion()
        #     print("  âœ… é¢„æµ‹æ¦‚ç‡åŠ æƒèåˆæ¨¡å‹åˆ›å»ºå®Œæˆ")
        # except Exception as e:
        #     print(f"  âŒ é¢„æµ‹æ¦‚ç‡åŠ æƒèåˆæ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
        
        # åˆ›å»ºç‰¹å¾çº§èåˆæ¨¡å‹ï¼ˆå·²æ³¨é‡Šï¼Œä¿ç•™ä»£ç å¤‡ç”¨ï¼‰
        # print(f"\nğŸ”§ åˆ›å»ºç‰¹å¾çº§èåˆæ¨¡å‹...")
        # try:
        #     self.create_feature_level_fusion()
        #     print("  âœ… ç‰¹å¾çº§èåˆæ¨¡å‹åˆ›å»ºå®Œæˆ")
        # except Exception as e:
        #     print(f"  âŒ ç‰¹å¾çº§èåˆæ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
        
        # æ›´æ–°æ¨¡å‹å­—å…¸
        self.models.update(optimized_models)
        
        # ç¡®å®šæœ€ä½³æ¨¡å‹ï¼ˆåŒ…æ‹¬ä¼˜åŒ–åçš„æ¨¡å‹ï¼‰
        if self.model_scores:
            best_model_name = max(self.model_scores.keys(), 
                                key=lambda x: self.model_scores[x]['f1_macro'])
            self.best_model = self.models[best_model_name]
            
            print(f"\nğŸ† è¶…å‚æ•°ä¼˜åŒ–åçš„æœ€ä½³æ¨¡å‹: {best_model_name}")
            print(f"F1-macroå¾—åˆ†: {self.model_scores[best_model_name]['f1_macro']:.4f}")
            
            if 'best_params' in self.model_scores[best_model_name]:
                print(f"æœ€ä½³å‚æ•°: {self.model_scores[best_model_name]['best_params']}")
        
        print(f"\nğŸ“Š å½“å‰å…±æœ‰ {len(self.models)} ä¸ªæ¨¡å‹:")
        for name in self.models.keys():
            f1_macro_score = self.model_scores[name]['f1_macro']
            print(f"  - {name}: F1-macro = {f1_macro_score:.4f}")
        
        return True

    def run_complete_diagnosis(self):
        """è¿è¡Œå®Œæ•´çš„è¯Šæ–­æµç¨‹"""
        print("ğŸš€ å¼€å§‹è½´æ‰¿æ•…éšœæ™ºèƒ½è¯Šæ–­åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        print("\n" + "="*50)
        print("ç¬¬1æ­¥: åŠ è½½ç‰¹å¾æ•°æ®")
        if not self.load_features():
            return False
        
        # 2. æ•°æ®é¢„å¤„ç†
        print("\n" + "="*50)
        print("ç¬¬2æ­¥: æ•°æ®é¢„å¤„ç†")
        if not self.preprocess_data():
            return False
        
        # 3. è®­ç»ƒæ ¸å¿ƒæ¨¡å‹
        print("\n" + "="*50)
        print("ç¬¬3æ­¥: è®­ç»ƒæ ¸å¿ƒæ¨¡å‹")
        self.train_core_models()
        
        # 4. å¯¹é€‰å®šæ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
        print("\n" + "="*50)
        print("ç¬¬4æ­¥: è¶…å‚æ•°ä¼˜åŒ–")
        self.optimize_selected_models()
        
        # 5. ç”Ÿæˆå¯è§†åŒ–ç»“æœ
        print("\n" + "="*50)
        print("ç¬¬5æ­¥: ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
        self.plot_model_comparison()
        self.plot_confusion_matrices()
        self.plot_roc_curves()
        self.plot_feature_importance()
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        print("\n" + "="*50)
        print("ç¬¬6æ­¥: ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š")
        self.generate_classification_report()
        
        # 7. ä¿å­˜å…¨éƒ¨è®­ç»ƒçš„æ¨¡å‹
        print("\n" + "="*50)
        print("ç¬¬7æ­¥: ä¿å­˜å…¨éƒ¨è®­ç»ƒçš„æ¨¡å‹")
        self.save_all_models()
        
        # 8. æ˜¾ç¤ºæœ€ç»ˆç»“æœæ€»ç»“
        print("\n" + "="*50)
        print("ç¬¬8æ­¥: æœ€ç»ˆç»“æœæ€»ç»“")
        self._print_final_summary()
        
        print("\nğŸ‰ è½´æ‰¿æ•…éšœæ™ºèƒ½è¯Šæ–­åˆ†æå®Œæˆï¼")
        return True


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè¯Šæ–­æ¨¡å‹
    model = EnhancedBearingDiagnosisModel()
    
    # è¿è¡Œå®Œæ•´è¯Šæ–­æµç¨‹
    success = model.run_complete_diagnosis()
    
    if success:
        print("\n=== è¯Šæ–­ç»“æœæ‘˜è¦ ===")
        print(f"è®­ç»ƒçš„æ¨¡å‹æ•°é‡: {len(model.models)}")
        print(f"æœ€ä½³æ¨¡å‹æ€§èƒ½:")
        
        best_scores = max(model.model_scores.values(), key=lambda x: x['f1_macro'])
        print(f"  å‡†ç¡®ç‡: {best_scores['accuracy']:.4f}")
        print(f"  F1-macro: {best_scores['f1_macro']:.4f}")
        print(f"  F1-weighted: {best_scores['f1_weighted']:.4f}")
        
        print(f"\næ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°: {model.results_dir}")
    else:
        print("è¯Šæ–­æµç¨‹æ‰§è¡Œå¤±è´¥ï¼")


if __name__ == "__main__":
    main()